\documentclass[12pt]{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}\usepackage{bm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[noEnd=false,indLines=false]{algpseudocodex}
\usepackage{colortbl}\usepackage{multirow}\usepackage{url}
\usepackage{placeins}
\usepackage{hhline}
%\usepackage{colonequals}
\usepackage{tikz}
\renewcommand{\topfraction}{1}
\renewcommand{\bottomfraction}{1}
\renewcommand{\textfraction}{0}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\R{ {\mathbb R}}\def\C{ {\mathbb C}}\def\N{ {\mathbb N}}
\def\P{{\mathbb P}}\def\Z{{\mathbb Z}}
\def\alt{\mathrel{\raisebox{-.75ex}{$\mathop{\sim}\limits^{\textstyle <}$}}}
\def\ctp{^{\rm H}}\def\dia{{\rm diag}}\def\fro{_{\rm F}}\def\trp{^{\rm T}}
\def\trace{\operatorname{tr}}
\def\adj{^{*}}
\def\inv{^{-1}}\def\itp{^{\rm -T}}\def\unv{{\bf e}}\def\exc{{\rm exc}}
\def\real{\mbox{\rm Re}}\def\imag{\mbox{\rm Imag}}
\def\qed{\rule{1.2ex}{1.2ex}}\def\conj#1{\overline{#1}}
\def\fl{\mbox{fl}}\def\op{\, \mbox{op}\,}\def\sign{\mbox{sign}}
\def\eqand{\qquad\mbox{and}\qquad}
\def\rank{{\rm rank}}
\def\range{{\cal R}}
\def\nullspace{{\cal N}}
\def\vec#1{{\bf #1}}
\def\T{{\rm T}}
\def\pT{{\prime\, \T}}
\def\H{{\rm H}}
\def\th#1{\tilde{\hat{#1}}}
\def\e#1{{\times}10^{#1}}
\def\diag{\mbox{diag}}
\newcommand{\minprobc}[3]{
  \begin{array}{ll}
    \mbox{min}_{#1} & #2 \\
    \rule{0pt}{1.1em}\mbox{subject to} & #3
  \end{array} }

\newenvironment{mtx}[1]{\left[\begin{array}{#1}}{\end{array}\right]}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

% Put paragraph indentation in enumerate environment.
\let\oldenumerate=\enumerate
\renewenvironment{enumerate}{\oldenumerate\parindent=1.5em}{\endlist}

\bibliographystyle{siam} \title{On the Inversion in the Solution of
  the Generalized Eigenvalue Problem} \author{Michael
  Stewart\thanks{Department of Mathematics and Statistics, Georgia
    State University, Atlanta GA 30303, {\tt mastewart@gsu.edu}}}
\begin{document}
\maketitle
\begin{abstract}
  Given matrices $A$ and $B$, this paper gives an error analysis of
  the solution of the generalized eigenvalue problem using a shift and
  invert strategy.  The analysis identifies circumstances under which
  the solution can be expected to satisfy useful error bounds.  The
  conditions are notably weaker than requiring that one of the
  matrices, or a shift of one of the matrices, be well conditioned.
  When the conditions of the formal analysis do not guarantee
  stability, an orthogonalization procedure is empirically shown to
  help restore stability in some cases.
\end{abstract}
% \begin{keywords}
%   product SVD, product $URV$, product singular values, rank deficiency
% \end{keywords}
% \begin{AMS}
%   65F, 65F15
% \end{AMS}
\pagestyle{myheadings}
\thispagestyle{plain}

\section{Background}
\label{sec:background}

In what follows $A$ and $B$ are $n\times n$ nonzero complex matrices
that are not necessarily hermitian.  Our goal is to solve the
generalized eigenvalue problem
\begin{equation}
  \label{eq:gen_eig1}
  A\vec{v} = \lambda B \vec{v}, \qquad \vec{v} \neq \vec{0}
\end{equation}
or, equivalently, the alternate formulation
\begin{equation}
  \label{eq:gen_eig2}
  \beta A \vec{v} = \alpha B \vec{v}, \qquad \vec{v}\neq \vec{0},
\end{equation}
where $\beta$ and $\alpha$ are not both zero and
$\lambda = \alpha/\beta$.  If $\beta = 0$, we set $\lambda = \infty$.
If $A$ and $B$ have a common null space, then any common null vector
would satisfy \eqref{eq:gen_eig1} or \eqref{eq:gen_eig2} for any
choice of $\lambda$ or $\alpha$ and $\beta$.  We therefore require
that the intersection of the null spaces of $A$ and $B$ be trivial.
The problem is fully defined by the pair $(A,B)$ and the eigenvalues
by $(\alpha, \beta)$.  When the context is clear, we will often simply
refer to eigenvalues and eigenvectors instead of generalized
eigenvalues and eigenvectors.

If $B$ is nonsingular, then \eqref{eq:gen_eig1} can be converted to
the ordinary eigenvalue problem by solving $BY = A$ and solving
$Y \vec{v} = \lambda \vec{v}$.  However, if $B$ is ill conditioned
then the computed $Y$ will have large errors and the computed
generalized eigenvalues can be inaccurate, even when they are well
conditioned \cite{govl:13, stew:01}.  If $B-\sigma A$ is well
conditioned, then one can instead solve $(B-\sigma A) Y = A$.  If the
eigenvalues of $Y$ are $\gamma_i$, then the eigenvalues in
\eqref{eq:gen_eig1} are $\lambda_i = \gamma_i/(1+\sigma\gamma_i)$.
However, it is not always possible to choose $\sigma$ so that
$B-\sigma A$ is well conditioned.  Even with the use of a shift, the
explicit conversion of the generalized eigenvalue problem to an
ordinary eigenvalue problem is not commonly recommended, except
perhaps in the use of iterative methods like the Arnoldi algorithm.

The $QZ$ algorithm \cite{most:73} instead solves the generalized
eigenvalue by computing orthogonal $Q$ and $Z$ for which
\begin{equation*}
  A = Q T_a Z^\H, \eqand B = QT_b Z^\H
\end{equation*}
where $T_a$ and $T_b$ are upper triangular.  If the $i$th diagonal
elements of $T_a$ and $T_b$ are $\alpha_i$ and $\beta_i$ respectively,
then each $(\alpha_i, \beta_i)$ is an eigenvalue of
\eqref{eq:gen_eig2}.  The decomposition is perfectly backward stable
so that for $i=1,\ldots, n$ the pairs $(\alpha_i, \beta_i)$ form the
set of eigenvalues of a pair of matrices close to $(A,B)$.  The
generalized eigenvectors can be obtained from $Z$ in a manner
analogous to what is done with a Schur decomposition for the ordinary
eigenvalue problem.  The only disadvantage of this method is the
increased cost of the $QZ$ algorithm relative to the cost of computing
a Schur decomposition of $B^{-1}A$, which can be significant.
However, given the possible instability associated with forming
$B^{-1}A$, the $QZ$ algorithm is the standard method for the dense
nonhermitian generalized eigenvalue problem.

In this paper, we revisit the use of inversion to explicitly convert
the generalized eigenvalue problem to an ordinary eigenvalue problem
and show that, with some modifications, the method can be made more
stable than might be expected from its checkered reputation.  The
approach involves two components.  First we consider a suitably chosen
shift $\sigma$ used to compute $X=(A-\sigma B)^{-1} B$ that under some
fairly restrictive assumptions on $\sigma$ can be shown to stabilize
the computation of the generalized eigenvalues of $(A,B)$ through the
solution of the ordinary eigenvalue problem for $X$.  In some cases,
the shift can be chosen to give residual bounds for the generalized
eigenvalues and eigenvectors, even when $A-\sigma B$ is severely ill
conditioned.  Second, since the assumptions on $\sigma$ can often be
difficult to satisfy when $B^{-1}A$ is nonnormal, we introduce a $QR$
factorization
\begin{equation*}
  \begin{bmatrix}
    A-\sigma B \\ B
  \end{bmatrix} =
  \begin{bmatrix}
    \hat{A} \\ \hat{B}
  \end{bmatrix} R
\end{equation*}
where the columns of
$\begin{bmatrix} \hat{A}^\H & \hat{B}^\H \end{bmatrix}^\H$ are
orthonormal.  The computation then proceeds as before by finding
eigenvalues $\theta_i$ and corresponding eigenvectors $\vec{u}_i$ of
\begin{equation*}
  \hat{X} = \hat{A}^{-1} \hat{B}.
\end{equation*}
It is easily seen that $\hat{X} = R X R^{-1}$.  The eigenvalues and
eigenvectors of \eqref{eq:gen_eig2} are then given by
$(1+\sigma \theta_i, \theta_i)$ and $\vec{v}_i = R^{-1} \vec{u}_i$.
The justification for the orthogonalization step is empirical.  We are
able to identify several mechanisms by which the orthogonalization
improves residuals for the computed eigenvalues and eigenvectors, but
one of those mechanisms involves grading in $\hat{X}$ for which the
$QR$ algorithm performs better than standard error bounds suggest.  It
is also not difficult to construct problems for which the
orthogonalization step fails to improve residuals.  Nevertheless, when
compared to the straightforward approach of forming $B^{-1}A$, the
method often works well to improve the stability of the method.

\section{Numerical Stability}
\label{sec:numerical-stability}

In this section, we ultimately prove residual bounds for computed
generalized eigenvalues and eigenvectors determined from eigenvalues
and eigenvectors of $(A-\sigma B)^{-1}B$.  There are some technical
concerns related to scaling and the magnitude of the shift $\sigma$,
but a rough summary of the results is that we can expect to have
useful error bounds when $\sigma$ is not in the
$\epsilon$-pseudospectrum of $B^{-1}A$ for small $\epsilon$.  However,
we do not want to assume that $B$ is invertible, so we make a small
adaptation of standard results on pseudospectra by considering a set
that suits our purposes and is equivalent to the pseudospectrum of
$B^{-1}A$ when $B$ is invertible.

Let
\begin{equation}
  \label{eq:S_def}
  \mathcal{S}(\sigma) =
  \mathcal{S}(A,B,\sigma) = 
  \left\{ E : \mbox{$A - \sigma B - B E$ is singular} \right\}.
\end{equation}
For $\epsilon \geq 0$ define
\begin{equation}
  \label{eq:pseudo_def}
  \Lambda(\epsilon) = \Lambda(A, B, \epsilon) =
  \left\{ \sigma \in \C: \mbox{there exists $E\in \mathcal{S}_\sigma$
      with $\|E\|_2\leq \epsilon$}
  \right\}.
\end{equation}
Clearly, $\Lambda(0)$ is the set of finite generalized eigenvalues of
$(A,B)$.  If $B$ is nonsingular, then $\Lambda(\epsilon)$ is the
$\epsilon$-pseudospectrum of $B^{-1} A$.  We are interested primarily
in a nonzero shift, in which case $\Lambda(\epsilon)$ has a natural
interpretation in terms of generalized eigenvalues even when $B$ is
singular.  For $\sigma \neq 0$ we have $\sigma\in\Lambda(\epsilon)$ if
and only if there exists $E$ with $\|E\|_2 \leq \epsilon/\sigma$ such
that $\sigma$ is a generalized eigenvalue of $(A, B(I-E))$.

Define
\begin{equation}
  \label{eq:delta_def}
  \delta(\sigma) = \delta(A, B, \sigma) = \inf \{ \epsilon :
  \mbox{$\epsilon \geq 0$ and $\sigma \in \Lambda_\epsilon$}\}
\end{equation}
The infimum is achieved and for some $\epsilon_0 =\delta(\sigma)$ for
which $\sigma \in \Lambda_{\epsilon_0}$ and
$\sigma \notin \Lambda_{\epsilon}$ for $\epsilon < \epsilon_0$.  Thus,
if $B$ is invertible, then there exists $E$ with
$\|E\|_2 = \delta(\sigma)$ that makes $\sigma$ an eigenvalue of
$B^{-1}A - E$ and this is not achieved for any $E$ of smaller norm.
Thus the quantity $\delta(\sigma)$ can be loosely viewed a distance of
the set of eigenvalues of $B^{-1}A$ from $\sigma$ in a pseudospectral
sense in which we are concerned with the absolute magnitude of
$\|E\|_2$ required to make $\sigma$ an eigenvalue of $B^{-1}A - E$.

Without assuming $B$ is invertible, if $\sigma \neq 0$, we can say
that there exists $E$ with $\|E\|_2= \delta(\sigma)/\sigma$ that makes
$\sigma$ a generalized eigenvalue of $(A, B(I-E))$ and this is also
not achieved for any $E$ of smaller norm.  We can view
$\delta(\sigma)/\sigma$ as a relative distance of the set of
generalized eigenvalues from $\sigma$ in a pseudospectral sense in
which we are concerned with the relative change to $B$ needed to make
$\sigma$ a generalized eigenvalue.  We refer to $\delta(\sigma)$ as
the pseudospectral distance and $\delta(\sigma)/\sigma$ as the
relative pseudospectral distance of $\sigma$ from a generalized
eigenvalue of $(A,B)$.  We have emphasized both absolute and relative
measures because they give slightly different perspectives on
magnitudes that we bound below and will later use in an error
analysis.

If $B$ is nonsingular and $\sigma$ is not an eigenvalue of $B^{-1}A$,
then it is well known that $\sigma\notin \Lambda_0$ is in the
$\epsilon$-pseudospectrum of $B^{-1}A$ if and only if
$\|(B^{-1}A - \sigma I)^{-1}\|_2\geq 1/\epsilon$.  This is equivalent
to
\begin{equation*}
  \delta(\sigma) = \frac{1}{\|(B^{-1}A - \sigma I)^{-1}\|_2}.
\end{equation*}
The following lemma reformulates this result to avoid assuming
invertibility of $B$.
\begin{lemma}
  \label{lm:X_norm_bound}
  With $\delta(\sigma)$ defined as in \eqref{eq:delta_def}, we have
  \begin{equation*}
     \delta(\sigma) 
     = 
     \left\{
       \begin{array}{ll}
         \frac{1}{\|(A-\sigma B)^{-1} B\|_2}, & \sigma \notin \Lambda_0 \\
         0, & \mbox{otherwise} \\
       \end{array}
     \right..
  \end{equation*}
\end{lemma}
\begin{proof}
  If $A-\sigma B$ is singular, it is clear from \eqref{eq:pseudo_def}
  and \eqref{eq:delta_def} that $\delta(\sigma)=0$.  If $A-\sigma B$
  is nonsingular we consider the generalized singular value
  decomposition
  \begin{equation*}
    A - \sigma B = S I U^\H,\eqand
    B = S D V^\H
  \end{equation*}
  where $S=(A-\sigma B)U$ is invertible,
  $(A-\sigma B)^{-1} B = U D V^\H$, $U$ and $V$ are unitary, and
  $D = \diag(d_1, \ldots, d_n)$ with $d_1 > 0$ and $d_k\geq 0$
  nonincreasing for $k=1,2,\ldots, n$.   Clearly
  $A-\sigma B - BE = S (I - D V^\H E U) U^\H$ is
  singular if and only if $(I - D V^\H E U)$ is singular which implies
  that $\|E\|_2 \|D\|_2 \geq 1$ and
  \begin{equation*}
    \delta(\sigma)
    \geq \frac{1}{d_1} =
    \frac{1}{\|(A-\sigma B)^{-1}B\|_2}.
  \end{equation*}
  Let  
  \begin{equation*}
    \hat{E} = \frac{1}{d_1}V \vec{e}_1 \vec{e}_1^\H U^\H.
  \end{equation*}
  Then
  \begin{equation*}
    A-\sigma B - B \hat{E} = S (I-\vec{e}_1 \vec{e}_1^\H) U^\H
  \end{equation*}
  is singular so that
  \begin{equation*}
    \delta(\sigma)
    \leq \|\hat{E}\|_2 
    = \frac{1}{d_1} = \frac{1}{\|(A-\sigma B)^{-1} B\|_2}.
  \end{equation*}
\end{proof}


We define
\begin{equation}
  \label{eq:Xdef}
  X \coloneqq (A-\sigma B)^{-1} B,
\end{equation}

\begin{equation}
  \label{eq:eta_def}
  \eta \coloneqq \frac{\|A-\sigma B\|_2}{\|B\|_2},
\end{equation}
\begin{equation}
  \label{eq:sigma0_def}
  \sigma_0 \coloneqq \sigma \frac{\|B\|_2}{\|A\|_2},
\end{equation}
and
\begin{equation}
  \label{eq:gamma_def}
  \gamma \coloneqq \frac{\|A\|_2}{\|A-\sigma B\|_2}.
\end{equation}
We are interested in results that do not depend on the scaling of $A$
or $B$.  To this end, we also define a scaled version
$\delta_0(\sigma_0)$ of $\delta(\sigma)$.  It is easily seen that
\begin{equation*}
  \delta_0(\sigma_0) \coloneqq \delta\left(\frac{A}{\|A\|_2}, \frac{B}{\|B\|_2}, \sigma_0 \right) =
  \frac{\|B\|_2}{\|A\|_2} \delta(A, B, \sigma)=\frac{\|B\|_2}{\|A\|_2} \delta(\sigma).
\end{equation*}

In considering the stability of using the eigenvector decomposition of $X$
to compute generalized eigenvalues of $(A,B)$, we will be particularly concerned with
the quantity $\eta \|X\|_2$, which is easily seen to satisfy
\begin{equation*}
  1 \leq \eta \|X\|_2 \leq \kappa_2(A-\sigma B).
\end{equation*}
The upper bound might give the impression that ill conditioning of
$A-\sigma B$ could lead to large $\eta \|X\|_2$.  While this can
happen, the following lemma shows that it does not happen if
$\sigma$ is not close to a generalized eigenvalue in a quantifiable pseudospectral
sense.
\begin{lemma}
  \label{lm:eta_X_bound}
  If $\sigma \notin \Lambda_0$, then
  \begin{align*}
    \eta \|X\|_2     
    & \leq (1+|\sigma_0|) \frac{1}{\delta_0(\sigma_0)} \\
    & = \left(1 + \frac{1}{|\sigma_0|}\right)
      \frac{1}{\delta_0(\sigma_0)/|\sigma_0|},
  \end{align*}
  where the equality in the second line holds if $\sigma \neq 0$.
\end{lemma}
\begin{proof}
  From $\sigma \notin \Lambda_0$ we have $\delta(\sigma)\neq 0$ so that
  \begin{equation*}
    \eta \|X\|_2
    \leq \frac{\|A\|_2 + |\sigma| \|B\|_2}{\|B\|_2} \frac{1}{\delta(\sigma)}
    = (1 + |\sigma_0|) \frac{\|A\|_2}{\|B\|_2} \frac{1}{\delta(\sigma)}.
  \end{equation*}
  The second line is immediate from the first.
\end{proof}

The lemma gives two versions of the same upper bound.  The second
suggests if $|\sigma_0|$ is not small, then the size of $\eta \|X\|_2$
is determined by the relative pseudospectral distance of the set of
generalized eigenvalues from $\sigma$.  If $|\sigma_0|$ is not large
and the scaled pseudospectral distance $\delta_0(\sigma_0)$
is not small, then we also expect $\eta \|X\|_2$ not to be large.  The
scaling of $\delta(\sigma)$ is the same as is applied to give the
scaled shift $\sigma_0$.  Both suggest that to have a useful bound on
$\eta \|X\|_2$, we wish to avoid small $\delta_0(\sigma_0)$.

Suppose that $\sigma \notin \Lambda(0)$ and that we have a Schur
decomposition $X = (A-\sigma B)^{-1} B = ZTZ^\H$.  We then have
\begin{equation*}
  (A -\sigma B) Z T = B Z,\qquad\mbox{or}\qquad
  A Z T = B Z (I +\sigma T).
\end{equation*}
If wec ompute a $QR$ factorization $(A -\sigma B)Z = QT_1$, then $T_1$
is nonsingular and $Q^\H (A-\sigma B) Z = T_1$ so that
$Q^\H BZ = T_1 T$.  This leads to a generalized Schur decomposition
\begin{equation*}
  Q^\H A Z = T_1(I+\sigma T),\eqand Q^\H BZ = T_1 T.
\end{equation*}
Since $T_1$ is nonsingular, the generalized eigenvalues of $(A,B)$ are
the same as those of $(I+\sigma T, T)$.  If the diagonal elements of
$T$ are $\theta_i$ for $i=1,\ldots n$, then the eigenvalues of $(A,B)$
are $(1+\sigma\theta_i, \theta_i)\neq (0,0)$.  The finite eigenvalues
are $\lambda_i = (1+\sigma \theta_i)/\theta_i$ for each
$\theta_i\neq 0$.  The first $k$ columns of $Z$ give an orthonormal
basis for a deflating subspace of $(A,B)$.  Eigenvectors can then be
computed in a manner that is similar to any other generalized Schur
decomposition.  Specifically, if $\vec{u}_i$ is an eigenvector for
$(I+\sigma T, T)$ corresponding to eigenvalue
$(1+\sigma\theta_i, \theta_i)$, then
\begin{equation*}
  \theta_i A Z \vec{u}_i 
  = \theta_i QT_1(I+\sigma T) \vec{u}_i
  = (1+\sigma \theta_i) Q T_1 T \vec{u}_i
  = (1+\sigma \theta_i) B Z \vec{u}_i
\end{equation*}
so that $\vec{v}_i = Z \vec{u}_i$ is an eigenvector for $(A,B)$.

The following shows that under reasonable assumptions on the numerical
errors associated with the computation of $X$ and the Schur
decomposition of $X$, this algorithm achieves small backward errors
when $|\sigma_0|$ is not large and $\eta \|X\|_2$ is not large.
\begin{theorem}
  \label{th:deflating_subspaces}
  Suppose that $A-\sigma B$ is nonsingular, and that there exist
  $a_n\geq 0$ and $b_n\geq 0$ depending solely on $n$ for which the
  computed $X$ satisfies
  \begin{equation}
    \label{eq:Xerror}
    \left\| (A-\sigma B) X - B \right\|_2 \leq u a_n \|A-\sigma B\|_2 \|X\|_2
  \end{equation}
  and the computed Schur factor $T$ of $X$ is nonsingular and satisfies
  \begin{equation}
    \label{eq:schur_error}
    \| X - \tilde{Z} T \tilde{Z}^\H \|_2 \leq u b_n \|X\|_2,
  \end{equation}
  where $\tilde{Z}$ is exactly unitary.  Then there exist $E$ and $F$
  satisfying
  \begin{equation*}
    (A+E)\tilde{Z} T = (B+F)\tilde{Z}(I+\sigma T)
  \end{equation*}
  with
  \begin{equation*}
    \left\|
      \begin{bmatrix}
        \frac{E}{\|A\|_2} & \frac{F}{\|B\|_2}
      \end{bmatrix}
    \right\|_2 \leq u (a_n + b_n)(1+|\sigma_0|) \eta \|X\|_2.
  \end{equation*}
\end{theorem}
\begin{proof}
  Define $G_0 = (A-\sigma B) X - B$ and $G_1 = X - \tilde{Z}T\tilde{Z}^\H$.  Then
  \begin{equation*}
    (A-\sigma B) \tilde{Z} T 
    = (A-\sigma B) X \tilde{Z} - (A-\sigma B) G_1 \tilde{Z}
    = B\tilde{Z} + G_0\tilde{Z} - (A-\sigma B) G_1 \tilde{Z}
  \end{equation*}
  so that
  \begin{equation*}
    A \tilde{Z} T= B \tilde{Z} (I+\sigma T) + G_0 \tilde{Z} - (A-\sigma B)G_1 \tilde{Z}.
  \end{equation*}
  Defining
  \begin{equation*}
    G = G_0 \tilde{Z} - (A-\sigma B)G_1 \tilde{Z}
  \end{equation*}
  we have
  \begin{equation*}
    \|G\|_2
    \leq u (a_n + b_n) \|A-\sigma B\|_2 \|X\|_2
    = u (a_n + b_n) \eta \|X\|_2 \|B\|_2.
  \end{equation*}
  We have $(A+E)\tilde{Z} = (B+F)\tilde{Z}(I+\sigma T)$ if and only if
  \begin{equation*}
    \|B\|_2 \begin{bmatrix}
      -\frac{E\tilde{Z}}{\|A\|_2} & \frac{F\tilde{Z}}{\|B\|_2}
    \end{bmatrix}
    \begin{bmatrix}
      -\|A\|_2 T / \|B\|_2 \\ I + \sigma T
    \end{bmatrix}
    = G.
  \end{equation*}
  If we define
  \begin{equation*}
    C
    =
    \begin{bmatrix}
      -\|A\|_2 T / \|B\|_2 \\ I + \sigma T
    \end{bmatrix}
  \end{equation*}
  then using the fact that $T$ is nonsingular and $A \neq 0$ implies
  that $C$ has linearly independent columns.  One choice for $E$ and
  $F$ that satisfies the above relation is then given by
  \begin{equation*}
    \begin{bmatrix}
      -\frac{E\tilde{Z}}{\|A\|_2} & \frac{F\tilde{Z}}{\|B\|_2}
    \end{bmatrix} = \frac{1}{\|B\|_2} G C^{\dagger}
  \end{equation*}
  where $C^{\dagger}$ is the pseudoinverse of $C$.  Clearly $\sigma_n(C)\neq 0$ and
  $\|C^\dagger\|_2 = 1/\sigma_n(C)$.  We consider two cases.  Given a
  vector $\vec{x}$ with $\|\vec{x}\|_2 = 1$, either
  \begin{equation*}
    \|T\vec{x}\|_2 \leq \frac{\|B\|_2}{(1+|\sigma_0|)\|A\|_2},
    \qquad\mbox{or}\qquad
    \|T\vec{x}\|_2 > \frac{\|B\|_2}{(1+|\sigma_0|)\|A\|_2}.
  \end{equation*}
  In the first case we have
  \begin{multline*}
    \|(I + \sigma T) \vec{x}\|_2
    \geq \|\vec{x}\|_2 - |\sigma| \|T\vec{x}\|
    \geq 1 - \frac{|\sigma| \|B\|_2}{(1+|\sigma_0|)\|A\|_2}\\
    = 1 - \frac{|\sigma_0| \|A\|_2}{(1+|\sigma_0|)\|A\|_2}
    = \frac{1}{1+|\sigma_0|}.
  \end{multline*}
  The second case immediately gives
  \begin{equation*}
    \frac{\|A\|_2\|T\vec{x}\|_2}{\|B\|_2}  > \frac{1}{1+|\sigma_0|}.
  \end{equation*}
  It follows that $\|C\vec{x}\| \geq 1/(1+|\sigma_0|)$ so that $\|C^{\dagger}\|_2\leq (1+|\sigma_0|)$ and
  \begin{equation*}
    \begin{bmatrix}
      \frac{E}{\|A\|_2} & \frac{F}{\|B\|_2}
    \end{bmatrix}
    \leq \frac{1+|\sigma_0|}{\|B\|_2} \|G\|_2
    \leq u(a_n+b_n) (1+|\sigma_0|)\eta \|X\|_2.
  \end{equation*}
\end{proof}
    \end{bmatrix} = \frac{1}{\|B\|_2} G C^{\dagger}
  \end{equation*}
  where $C^{\dagger}$ is the pseudoinverse of $C$.  Clearly $\sigma_n(C)\neq 0$ and
  $\|C^\dagger\|_2 = 1/\sigma_n(C)$.  We consider two cases.  Given a
  vector $\vec{x}$ with $\|\vec{x}\|_2 = 1$, either
  \begin{equation*}
    \|T\vec{x}\|_2 \leq \frac{\|B\|_2}{(1+|\sigma_0|)\|A\|_2},
    \qquad\mbox{or}\qquad
    \|T\vec{x}\|_2 > \frac{\|B\|_2}{(1+|\sigma_0|)\|A\|_2}.
  \end{equation*}
  In the first case we have
  \begin{multline*}
    \|(I + \sigma T) \vec{x}\|_2
    \geq \|\vec{x}\|_2 - |\sigma| \|T\vec{x}\|
    \geq 1 - \frac{|\sigma| \|B\|_2}{(1+|\sigma_0|)\|A\|_2}\\
    = 1 - \frac{|\sigma_0| \|A\|_2}{(1+|\sigma_0|)\|A\|_2}
    = \frac{1}{1+|\sigma_0|}.
  \end{multline*}
  The second case immediately gives
  \begin{equation*}
    \frac{\|A\|_2\|T\vec{x}\|_2}{\|B\|_2}  > \frac{1}{1+|\sigma_0|}.
  \end{equation*}
  It follows that $\|C\vec{x}\| \geq 1/(1+|\sigma_0|)$ so that $\|C^{\dagger}\|_2\leq (1+|\sigma_0|)$ and
  \begin{equation*}
    \begin{bmatrix}
      \frac{E}{\|A\|_2} & \frac{F}{\|B\|_2}
    \end{bmatrix}
    \leq \frac{1+|\sigma_0|}{\|B\|_2} \|G\|_2
    \leq u(a_n+b_n) (1+|\sigma_0|)\eta \|X\|_2.
  \end{equation*}
\end{proof}




\bibliography{/home/mas/work/bib/ref}


\end{document}
