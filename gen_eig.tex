\documentclass[12pt]{article}
\usepackage[bookmarks=true]{hyperref}
% \usepackage{refcheck}
\usepackage[margin=1.25in]{geometry}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}\usepackage{bm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[noEnd=false,indLines=false]{algpseudocodex}
\usepackage{colortbl}\usepackage{multirow}\usepackage{url}
\usepackage{placeins}
\usepackage{hhline}
%\usepackage{colonequals}
\usepackage{tikz}
\renewcommand{\topfraction}{1}
\renewcommand{\bottomfraction}{1}
\renewcommand{\textfraction}{0}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\R{ {\mathbb R}}\def\C{ {\mathbb C}}\def\N{ {\mathbb N}}
\def\P{{\mathbb P}}\def\Z{{\mathbb Z}}
\def\alt{\mathrel{\raisebox{-.75ex}{$\mathop{\sim}\limits^{\textstyle <}$}}}
\def\ctp{^{\rm H}}\def\dia{{\rm diag}}\def\fro{_{\rm F}}\def\trp{^{\rm T}}
\def\trace{\operatorname{tr}}
\def\adj{^{*}}
\def\inv{^{-1}}\def\itp{^{\rm -T}}\def\unv{{\bf e}}\def\exc{{\rm exc}}
\def\real{\mbox{\rm Re}}\def\imag{\mbox{\rm Imag}}
\def\qed{\rule{1.2ex}{1.2ex}}\def\conj#1{\overline{#1}}
\def\fl{\mbox{fl}}\def\op{\, \mbox{op}\,}\def\sign{\mbox{sign}}
\def\eqand{\qquad\mbox{and}\qquad}
\def\rank{{\rm rank}}
\def\range{{\cal R}}
\def\nullspace{{\cal N}}
\def\vec#1{{\bf #1}}
\def\T{{\rm T}}
\def\pT{{\prime\, \T}}
\def\H{{\rm H}}
\def\th#1{\tilde{\hat{#1}}}
\def\e#1{{\times}10^{#1}}
\def\diag{\mbox{diag}}
\def\mcal{\mathcal}

\newenvironment{mtx}[1]{\left[\begin{array}{#1}}{\end{array}\right]}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

% Put paragraph indentation in enumerate environment.
\let\oldenumerate=\enumerate
\renewenvironment{enumerate}{\oldenumerate\parindent=1.5em}{\endlist}

\bibliographystyle{siam} \title{An Error Analysis of Explicit Shift
  and Invert Solution of the Dense Generalized Eigenvalue Problem}
\author{Michael Stewart\thanks{Department of Mathematics and
    Statistics, Georgia State University, Atlanta GA 30303, {\tt
      mastewart@gsu.edu}}}
\begin{document}
\maketitle
\begin{abstract}
  Given matrices $A$ and $B$, this paper gives an error analysis of
  the solution of the generalized eigenvalue problem using a shift and
  invert strategy.  The analysis identifies circumstances under which
  the solution can be expected to satisfy useful error bounds.  The
  conditions are notably weaker than requiring that one of the
  matrices, or a shift of one of the matrices, be well conditioned.
  When the conditions of the formal analysis do not guarantee
  stability, an orthogonalization procedure is empirically shown to
  help restore stability in some cases.
\end{abstract}
% \begin{keywords}
%   product SVD, product $URV$, product singular values, rank deficiency
% \end{keywords}
% \begin{AMS}
%   65F, 65F15
% \end{AMS}
\pagestyle{myheadings}
\thispagestyle{plain}

\section{Background}
\label{sec:background}

In what follows $A$ and $B$ are $n\times n$ nonzero complex matrices
that are not necessarily hermitian.  Our goal is to solve the
generalized eigenvalue problem
\begin{equation}
  \label{eq:gen_eig1}
  A\vec{v} = \lambda B \vec{v}, \qquad \vec{v} \neq \vec{0}
\end{equation}
or, equivalently, the alternate formulation
\begin{equation}
  \label{eq:gen_eig2}
  \beta A \vec{v} = \alpha B \vec{v}, \qquad \vec{v}\neq \vec{0},
\end{equation}
where $\beta$ and $\alpha$ are not both zero and
$\lambda = \alpha/\beta$.  If $\beta = 0$, we let $\lambda = \infty$.
If $A$ and $B$ have a common null space, then any common null vector
would satisfy \eqref{eq:gen_eig1} or \eqref{eq:gen_eig2} for any
choice of $\lambda$ or $\alpha$ and $\beta$.  We therefore require
that the intersection of the null spaces of $A$ and $B$ be trivial.
The problem is fully defined by the pair $(A,B)$ and the eigenvalues
by $(\alpha, \beta)$.  When the context is clear, we will often simply
refer to eigenvalues and eigenvectors instead of generalized
eigenvalues and eigenvectors.

If $B$ is nonsingular, then \eqref{eq:gen_eig1} can be converted to
the ordinary eigenvalue problem by solving $BY = A$ and solving
$Y \vec{v} = \lambda \vec{v}$.  However, if $B$ is ill conditioned
then the computed $Y$ will have large errors and the computed
generalized eigenvalues can be inaccurate, even when they are well
conditioned \cite{govl:13, stew:01}.  If $B-\sigma A$ is well
conditioned, then one can instead solve $(B-\sigma A) Y = A$.  If the
eigenvalues of $Y$ are $\phi_i$, then the eigenvalues in
\eqref{eq:gen_eig1} are $\lambda_i = \phi_i/(1+\sigma\phi_i)$.
However, it is not always possible to choose $\sigma$ so that
$B-\sigma A$ is well conditioned.  Even with the use of a shift, the
explicit conversion of the generalized eigenvalue problem to an
ordinary eigenvalue problem is not commonly recommended, except
perhaps in the use of iterative methods like the Arnoldi algorithm.

The $QZ$ algorithm \cite{most:73} instead solves the generalized
eigenvalue by computing orthogonal $Q$ and $Z$ for which
\begin{equation*}
  A = Q T_a Z^\H, \eqand B = QT_b Z^\H
\end{equation*}
where $T_a$ and $T_b$ are upper triangular.  If the $i$th diagonal
elements of $T_a$ and $T_b$ are $\alpha_i$ and $\beta_i$ respectively,
then each $(\alpha_i, \beta_i)$ is an eigenvalue of
\eqref{eq:gen_eig2}.  The decomposition is perfectly backward stable
so that for $i=1,\ldots, n$ the pairs $(\alpha_i, \beta_i)$ form the
set of eigenvalues of a pair of matrices close to $(A,B)$.  The
generalized eigenvectors can be obtained from $Z$ in a manner
analogous to what is done with a Schur decomposition for the ordinary
eigenvalue problem.  The only disadvantage of this method is the
increased cost of the $QZ$ algorithm relative to the cost of computing
a Schur decomposition of $B^{-1}A$, which can be significant.
However, given the possible instability associated with forming
$B^{-1}A$, the $QZ$ algorithm is the standard method for the dense
nonhermitian generalized eigenvalue problem.

In this paper, we revisit the use of inversion to explicitly convert
the generalized eigenvalue problem to an ordinary eigenvalue problem
and show that, with some modifications, the method can be made more
stable than might be expected from its checkered reputation.  The
approach involves two components.  First we consider a suitably chosen
shift $\sigma$ used to compute $X=(A-\sigma B)^{-1} B$ that under some
fairly restrictive assumptions on $\sigma$ can be shown to stabilize
the computation of the generalized eigenvalues of $(A,B)$ through the
solution of the ordinary eigenvalue problem for $X$.  In some cases,
the shift can be chosen to give residual bounds for the generalized
eigenvalues and eigenvectors, even when $A-\sigma B$ is severely ill
conditioned.  Second, since the assumptions on $\sigma$ can often be
difficult to satisfy when $B^{-1}A$ is nonnormal, we introduce a $QR$
factorization
\begin{equation*}
  \begin{bmatrix}
    A-\sigma B \\ B
  \end{bmatrix} =
  \begin{bmatrix}
    \hat{A} \\ \hat{B}
  \end{bmatrix} R
\end{equation*}
where the columns of
$\begin{bmatrix} \hat{A}^\H & \hat{B}^\H \end{bmatrix}^\H$ are
orthonormal.  The computation then proceeds as before by finding
eigenvalues $\theta_i$ and corresponding eigenvectors $\vec{u}_i$ of
\begin{equation*}
  \hat{X} = \hat{A}^{-1} \hat{B}.
\end{equation*}
It is easily seen that $\hat{X} = R X R^{-1}$.  The eigenvalues and
eigenvectors of \eqref{eq:gen_eig2} are then given by
$(1+\sigma \theta_i, \theta_i)$ and $\vec{v}_i = R^{-1} \vec{u}_i$.
The justification for the orthogonalization step is empirical.  We are
able to identify several mechanisms by which the orthogonalization
improves residuals for the computed eigenvalues and eigenvectors, but
one of those mechanisms involves grading in $\hat{X}$ for which the
$QR$ algorithm performs better than standard error bounds suggest.  It
is also not difficult to construct problems for which the
orthogonalization step fails to improve residuals.  Nevertheless, when
compared to the straightforward approach of forming $B^{-1}A$, the
method often works well to improve the stability of the method.

\section{Choosing a Shift}
\label{sec:shift}

In this section, we consider the choice of shift.  There are some
complications related to scaling and the magnitude of the shift
$\sigma$, but a rough summary is that we can expect to have useful
error bounds when $\sigma$ is not in the $\epsilon$-pseudospectrum of
$B^{-1}A$ for small $\epsilon$.  However, we do not want to assume
that $B$ is invertible, so we adapt some standard results on
pseudospectra by considering a set that suits our purposes and is
equivalent to the pseudospectrum of $B^{-1}A$ when $B$ is invertible.


For $B\neq 0$, let
\begin{equation}
  \label{eq:S_def}
  \mathcal{S}(\sigma) =
  \mathcal{S}(A,B,\sigma) = 
  \left\{ E \mid \mbox{$A - \sigma B - B E$ is singular} \right\}.
\end{equation}
With the assumption that $B\neq 0$, the set $\mcal{S}(\sigma)$ is
nonempty.  This can be shown by noting that if $A-\sigma B$ is
singular, then $E=0$ is in $\mcal{S}(\sigma)$.  If $A-\sigma B$ is
nonsingular and $B\vec{y}\neq \vec{0}$ for $\|\vec{y}\|_2 = 1$, we
have $(A-\sigma B) \vec{x} = B\vec{y}$ for some $\vec{x}\neq \vec{0}$.
The matrix $E = \vec{y}\vec{x}^\H/\|\vec{x}\|_2^2$ is then in
$\mcal{S}(\sigma)$.  The set is closed and contains a matrix $E$
for which $\|E\|_2$ is a minimum.  We can therefore define
\begin{equation}
  \label{eq:delta_def}
  \delta(\sigma) = \delta(A,B,\sigma) =
  \min_{E\in \mcal{S}(\sigma)} \|E\|_2
\end{equation}
knowing that there is some $E_0\in \mcal{S}(\sigma)$ for which
$\|E_0\|_2 = \delta(\sigma)$.  

If $B$ is nonsingular, then $E\in \mcal{S}(\sigma)$ if and only if
$B^{-1}A - \sigma I -E$ is singular.  If $B^{-1}A$ is normal then it
is easily seen that
\begin{equation}
  \label{eq:normal_delta}
  \delta(\sigma) = \frac{1}{\min_i |\lambda_i - \sigma|}
\end{equation}
where the minimum is taken over all generalized eigenvalues of $(A,B)$.

For $\epsilon \geq 0$ and nonsingular $B$, we can define the pseudospectrum of $B^{-1}A$ as
\begin{equation}
  \label{eq:pseudo_def}
  \Lambda(\epsilon) = \Lambda(A, B, \epsilon) =
  \left\{ \sigma \in \C \mid \mbox{there exists $E\in \mathcal{S}_\sigma$
      with $\|E\|_2\leq \epsilon$}
  \right\}.
\end{equation}
This definition is formulated so that defines $\Lambda(\epsilon)$ even
when $B$ is singular.  The set of finite generalized eigenvalues of
$(A,B)$ is given by $\Lambda(0)$ and $\Lambda(\epsilon)$ for
$\epsilon \geq 0$ contains the finite generalized eigenvalues of $(A,B)$.  We
also have $\sigma \in \Lambda(\epsilon)$ if and only if
$\delta(\sigma)\leq \epsilon$.

We refer to $\delta(\sigma)$ as the pseudospectral distance of
$\sigma$ from a generalized eigenvalue of $(A,B)$.  It measures the
smallest $\|E\|_2$ for which $\sigma$ is an eigenvalue of $(A-BE, B)$.
This is equivalent to $\sigma$ being an eigenvalue of
$(A, B(I-E/\sigma))$.  Thus $\delta(\sigma)/|\sigma|$ gives the
smallest $\|E\|_2$ for which $\sigma$ is an eigenvalue of
$(A, B(I-E))$.

If $B$ is nonsingular and $\sigma$ is not an eigenvalue of $B^{-1}A$,
then it is well known that $\sigma$ is in the
$\epsilon$-pseudospectrum of $B^{-1}A$ if and only if
$\|(B^{-1}A - \sigma I)^{-1}\|_2\geq 1/\epsilon$.  This is equivalent
to
\begin{equation*}
  \delta(\sigma) = \frac{1}{\|(B^{-1}A - \sigma I)^{-1}\|_2}.
\end{equation*}
The following lemma reformulates this result to avoid assuming
invertibility of $B$.
\begin{lemma}
  \label{lm:X_norm_bound}
  With $\delta(\sigma)$ defined as in \eqref{eq:delta_def}, we have
  \begin{equation*}
     \delta(\sigma) 
     = 
     \left\{
       \begin{array}{ll}
         \frac{1}{\|(A-\sigma B)^{-1} B\|_2}, & \sigma \notin \Lambda(0) \\
         0, & \mbox{otherwise} \\
       \end{array}
     \right..
  \end{equation*}
\end{lemma}
\begin{proof}
  If $A-\sigma B$ is singular, it is clear from \eqref{eq:pseudo_def}
  and \eqref{eq:delta_def} that $\delta(\sigma)=0$.  If $A-\sigma B$
  is nonsingular we consider the generalized singular value
  decomposition
  \begin{equation*}
    A - \sigma B = S I U^\H,\eqand
    B = S D V^\H
  \end{equation*}
  where $S=(A-\sigma B)U$ is invertible,
  $(A-\sigma B)^{-1} B = U D V^\H$, $U$ and $V$ are unitary, and
  $D = \diag(d_1, \ldots, d_n)$ with $d_1 > 0$ and $d_k\geq 0$
  nonincreasing for $k=1,2,\ldots, n$.   Clearly
  $A-\sigma B - BE = S (I - D V^\H E U) U^\H$ is
  singular if and only if $(I - D V^\H E U)$ is singular which implies
  that $\|E\|_2 \|D\|_2 \geq 1$ and
  \begin{equation*}
    \delta(\sigma)
    \geq \frac{1}{d_1} =
    \frac{1}{\|(A-\sigma B)^{-1}B\|_2}.
  \end{equation*}
  Let  
  \begin{equation*}
    \hat{E} = \frac{1}{d_1}V \vec{e}_1 \vec{e}_1^\H U^\H.
  \end{equation*}
  Then
  \begin{equation*}
    A-\sigma B - B \hat{E} = S (I-\vec{e}_1 \vec{e}_1^\H) U^\H
  \end{equation*}
  is singular so that
  \begin{equation*}
    \delta(\sigma)
    \leq \|\hat{E}\|_2 
    = \frac{1}{d_1} = \frac{1}{\|(A-\sigma B)^{-1} B\|_2}.
  \end{equation*}
\end{proof}

If $(A-\sigma B)^{-1}B$ is normal then we have
$(A-\sigma B)^{-1}B = Z \Theta Z^\H$ where $Z$ is unitary and $\Theta$
is diagonal with $\theta_i = 1/(\lambda_i - \sigma)$ for
$i=1,2,\ldots, n$ on its diagonal.  In this case we again have
\eqref{eq:normal_delta}, but now assuming invertibility of
$A-\sigma B$ instead of $B$.  Without an assumption of normality, we
expect $\delta(\sigma)$ to be larger than the right-hand side of
\eqref{eq:normal_delta}.

Define
\begin{equation}
  \label{eq:Xdef}
  X = (A-\sigma B)^{-1} B,
\end{equation}

\begin{equation}
  \label{eq:eta_def}
  \eta = \frac{\|A-\sigma B\|_2}{\|B\|_2},
\end{equation}
and
\begin{equation}
  \label{eq:sigma0_def}
  \sigma_0 = \sigma \frac{\|B\|_2}{\|A\|_2},
\end{equation}
The quantities $\eta$ and $\sigma_0$ are defined with the goal of
scaling our bounds so that they are independent of how $A$ and $B$ are
scaled.  To this end, we also define
\begin{equation*}
  \delta_0(\sigma_0) \coloneqq \delta\left(\frac{A}{\|A\|_2}, \frac{B}{\|B\|_2}, \sigma_0 \right) =
  \frac{\|B\|_2}{\|A\|_2} \delta(A, B, \sigma)=\frac{\|B\|_2}{\|A\|_2} \delta(\sigma).
\end{equation*}

In considering the stability of using the eigenvector decomposition of $X$
to compute generalized eigenvalues of $(A,B)$, we will be particularly concerned with
the quantity $\eta \|X\|_2$, which is easily seen to satisfy
\begin{equation*}
  1 \leq \eta \|X\|_2 \leq \kappa_2(A-\sigma B).
\end{equation*}
The upper bound might give the impression that ill conditioning of
$A-\sigma B$ could lead to large $\eta \|X\|_2$.  While this can happen,
a more useful bound is given in terms of $\delta(\sigma_0)$.
\begin{lemma}
  \label{lm:eta_X_bound}
  If $\sigma \notin \Lambda(0)$, then
  \begin{equation*}
    \eta \|X\|_2 \leq \frac{1}{\delta_0(\sigma_0)} + \frac{|\sigma_0|}{\delta_0(\sigma_0)}.
  \end{equation*}
\end{lemma}
\begin{proof}
  From $\sigma \notin \Lambda(0)$ we have $\delta(\sigma)\neq 0$ so that
  \begin{equation*}
    \eta \|X\|_2
    \leq \frac{\|A\|_2 + |\sigma| \|B\|_2}{\|B\|_2} \frac{1}{\delta(\sigma)}
    = (1 + |\sigma_0|) \frac{\|A\|_2}{\|B\|_2} \frac{1}{\delta(\sigma)}.
  \end{equation*}
\end{proof}

\section{Algorithm and Error Analysis}
\label{sec:algor-error-analys}

We now turn our attention to the computation of a generalized Schur
decomposition of $(A,B)$ from a schur decomposition of
$X = (A-\sigma B)^{-1} B$.  Suppose that $\sigma \notin \Lambda(0)$
and that we have a Schur decomposition $X = ZTZ^\H$.  We then have
\begin{equation*}
  (A -\sigma B) Z T = B Z,\qquad\mbox{or}\qquad
  A Z T = B Z (I +\sigma T).
\end{equation*}
If we compute a $QR$ factorization $(A -\sigma B)Z = QT_1$, then $T_1$
is nonsingular and $Q^\H (A-\sigma B) Z = T_1$ so that
$Q^\H BZ = T_1 T$.  This leads to a generalized Schur decomposition
\begin{equation*}
  Q^\H A Z = T_1(I+\sigma T),\eqand Q^\H BZ = T_1 T.
\end{equation*}
Since $T_1$ is nonsingular, the generalized eigenvalues of $(A,B)$ are
the same as those of $(I+\sigma T, T)$.  If the diagonal elements of
$T$ are $\theta_i$ for $i=1,\ldots n$, then the eigenvalues of $(A,B)$
are $(1+\sigma\theta_i, \theta_i)\neq (0,0)$.  The finite eigenvalues
are $\lambda_i = (1+\sigma \theta_i)/\theta_i$ for each
$\theta_i\neq 0$.  The first $k$ columns of $Z$ give an orthonormal
basis for a deflating subspace of $(A,B)$.  This approach leads to
Algorithm~\ref{alg:gen_schur}.

If needed, eigenvectors can be computed in a manner that is similar to
any other generalized Schur decomposition.  Specifically, if
$\vec{u}_i$ is an eigenvector for $(I+\sigma T, T)$ corresponding to
eigenvalue $(1+\sigma\theta_i, \theta_i)$, then
\begin{equation*}
  \theta_i A Z \vec{u}_i 
  = \theta_i QT_1(I+\sigma T) \vec{u}_i
  = (1+\sigma \theta_i) Q T_1 T \vec{u}_i
  = (1+\sigma \theta_i) B Z \vec{u}_i
\end{equation*}
so that $\vec{v}_i = Z \vec{u}_i$ is an eigenvector for $(A,B)$.  

\begin{algorithm}
\caption{Shift and Invert Schur Decomposition}
\label{alg:gen_schur}
\begin{algorithmic}
\Function{GenSchur}{$A, B, \sigma$}
\State $\breve{A} \gets A - \sigma B$
\State Solve: $\breve{A}X = B$
\State Factor: $X = Z T Z^{\H}$
\State $W \gets \breve{A}Z$
\State Factor: $W = Q T_1$
\State \Return $(T, T_1, Q, Z)$
\EndFunction
\end{algorithmic}
\end{algorithm}

The following lemma is useful in the derivation of backward error
bounds for Algorithm~\ref{alg:gen_schur}.
\begin{lemma}
  \label{lm:pseudo_inv}
  For a given shift $\sigma$ with $\sigma_0$ given by
  \eqref{eq:sigma0_def} and matrix $T$, let
  \begin{equation*}
    C
    =
    \begin{bmatrix}
      -\|A\|_2 T / \|B\|_2 \\ I + \sigma T
    \end{bmatrix}.
  \end{equation*}
  Then
  \begin{equation*}
    \|C^{\dagger}\|_2\leq (1+|\sigma_0|)
  \end{equation*}.
\end{lemma}
\begin{proof}
  We seek a lower bound on the smallest singular value of $C$.  We
  consider two cases.  Given a vector $\vec{x}$ with
  $\|\vec{x}\|_2 = 1$, either
  \begin{equation*}
    \|T\vec{x}\|_2 \leq \frac{\|B\|_2}{(1+|\sigma_0|)\|A\|_2},
    \qquad\mbox{or}\qquad
    \|T\vec{x}\|_2 > \frac{\|B\|_2}{(1+|\sigma_0|)\|A\|_2}.
  \end{equation*}
  In the first case we have
  \begin{multline*}
    \|(I + \sigma T) \vec{x}\|_2
    \geq \|\vec{x}\|_2 - |\sigma| \|T\vec{x}\|
    \geq 1 - \frac{|\sigma| \|B\|_2}{(1+|\sigma_0|)\|A\|_2}\\
    = 1 - \frac{|\sigma_0| \|A\|_2}{(1+|\sigma_0|)\|A\|_2}
    = \frac{1}{1+|\sigma_0|}.
  \end{multline*}
  The second case immediately gives
  \begin{equation*}
    \frac{\|A\|_2\|T\vec{x}\|_2}{\|B\|_2}  > \frac{1}{1+|\sigma_0|}.
  \end{equation*}
  It follows that $\|C\vec{x}\| \geq 1/(1+|\sigma_0|)$ for arbitrary
  $\vec{x}$ with $\|\vec{x}\|_2=1$ so that
  $\sigma_n(C)\geq 1/(1+|\sigma_0|)$ and
  $\|C^{\dagger}\|_2\leq (1+|\sigma_0|)$.
\end{proof}

\begin{theorem}
  \label{th:generalized_schur}
  Suppose that $A-\sigma B$ is nonsingular, and that there exist
  $a_n$, $b_n$, $c_n$, $d_n$, $e_n$, $f_n$, and $g_n$ depending solely
  on $n$ for which
  \begin{equation}
    \label{eq:shift_error}
    \|\breve{A} - (A-\sigma B)\|_2 \leq u a_n (\|A-\sigma B\|_2 + |\sigma| \|B\|_2),
  \end{equation}
  \begin{equation}
    \label{eq:Xerror}
    \left\| \breve{A} X - B \right\|_2 
    \leq u b_n \|\breve{A}\|_2\|X\|_2,
  \end{equation}
  \begin{equation}
    \label{eq:schur_error}
    \| X - \tilde{Z} T \tilde{Z}^\H \|_2 \leq u c_n \|X\|_2,\qquad
    \|Z - \tilde{Z}\|_2 \leq u d_n,
  \end{equation}
  \begin{equation}
    \label{eq:AZerror}
    \|\breve{A} Z - W\|_2 \leq u e_n \|\breve{A}\|_2,
  \end{equation}
  \begin{equation}
    \label{eq:QRerror}
    \|\tilde{Q}T_1 - W\|_2 \leq u f_n \|W\|_2, \eqand
    \|\tilde{Q} - Q\|_2 \leq ug_n ,
  \end{equation}
  where $\tilde{Q}$ and $\tilde{Z}$ are unitary, $T_1$ and $T$ are
  upper triangular, and $T$ is nonsingular.  Then there exist $E$ and
  $F$ satisfying
  \begin{equation*}
    \tilde{Q}^\H (A+E) \tilde{Z} = T_1(I+\sigma T),\eqand \tilde{Q}^\H (B+F) \tilde{Z}
    = T_1 T.
  \end{equation*}
  with
  \begin{equation*}
    \frac{\|E\|_2}{\|A\|_2} \leq
    u \Big(|\sigma_0| (b_n + c_n + d_n + e_n + f_n) \eta \|X\|_2 + (1+|\sigma_0|)(2a_n + d_n+e_n+f_n)\Big) +O(u^2)    
  \end{equation*}
  and
  \begin{equation*}
    \frac{\|F\|_2}{\|B\|_2} \leq
    u \left( b_n + c_n +d_n +e_n + f_n\right) \eta \|X\|_2 + O(u^2).
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $\breve{A} X - B = G_1$, $X - \tilde{Z}T \tilde{Z}^\H = G_2$,
  $Z - \tilde{Z} = G_3$, $\breve{A}Z - W = G_4$, and
  $\tilde{Q}T_1 - W = G_5$.  The first two of these identities give
  \begin{equation*}
    \breve{A} \tilde{Z} T = B\tilde{Z} + G_1 \tilde{Z} - \breve{A}G_2 \tilde{Z}.
  \end{equation*}
  Continuing with various substitutions, we have
  \begin{equation*}
    \breve{A} Z T = B\tilde{Z} + G_1 \tilde{Z} - \breve{A}G_2 \tilde{Z} + \breve{A} G_3 T,
  \end{equation*}
  \begin{equation*}
    W T = B\tilde{Z} + G_1 \tilde{Z} - \breve{A}G_2 \tilde{Z} + \breve{A} G_3 T - G_4 T,
  \end{equation*}
  and
  \begin{equation*}
    \tilde{Q} T_1 T = B\tilde{Z} + G_1 \tilde{Z} - \breve{A}G_2 \tilde{Z} + \breve{A} G_3 T - G_4 T + G_5 T.
  \end{equation*}
  This implies that
  \begin{equation*}
    B + F = \tilde{Q} T_1 T \tilde{Z}^\H
  \end{equation*}
  where
  \begin{align*}
    \|F\|_2
    & \leq \|G_1\|_2 + \|\breve{A}\|_2 \|G_2\|_2 + \|\breve{A}\|_2 \|G_3\|_2 \|T\|_2 +
      \|G_4\|_2 \|T\|_2 + \|G_5\|_2 \|T\|_2 \\
    & = \|G_1\|_2 + \|\breve{A}\|_2 \|G_2\|_2 + \|\breve{A}\|_2 \|G_3\|_2 \|X\|_2 +
      \|G_4\|_2 \|X\|_2 + \|G_5\|_2 \|X\|_2  + O(u^2) \\
    & \leq u ( b_n \|\breve{A}\|_2 \|X\|_2 + c_n \|\breve{A}\|_2 \|X\|_2 + d_n \|\breve{A}\|_2 \|X\|_2\\
    & \qquad + e_n \|\breve{A}\|_2 \|X\|_2 + f_n \|W\|_2 \|X\|_2) +O(u^2) \\
    & = u \left( b_n + c_n +d_n +e_n + f_n\right) \|A-\sigma B\|_2 \|X\|_2 + O(u^2) \\
    & = u \left( b_n + c_n +d_n +e_n + f_n\right) \eta \|X\|_2 \|B\|_2 + O(u^2).
  \end{align*}
  
  Let $\breve{A} = A-\sigma B + G_0$.  Considering errors on $A$, we
  start with $W - G_5 = \tilde{Q}T_1$ and continue with substitutions
  \begin{equation*}
    \breve{A}Z - G_4 - G_5 = \tilde{Q}T_1,
  \end{equation*}
  \begin{equation*}
    (A-\sigma B + G_0)Z - G_4 - G_5 = \tilde{Q}T_1,
  \end{equation*}
  and
  \begin{equation*}
    (A-\sigma B + G_0)\tilde{Z} - G_4 + (A-\sigma B+G_0) G_3 - G_5  = \tilde{Q}T_1.
  \end{equation*}
  This implies that
  \begin{multline*}
    \tilde{Q}^\H \left(A + \sigma F + G_0 + \tilde{Q} \left[(A-\sigma B+G_0) G_3- G_4 -G_5\right] \tilde{Z}^\H\right)
    \tilde{Z} \\
    = T_1 + \sigma \tilde{Q}^\H(B+F) \tilde{Z} = T_1 (I+\sigma T),
  \end{multline*}
  or
  \begin{equation*}
    \tilde{Q}^\H (A+E) \tilde{Z} = T_1(I+\sigma T)
  \end{equation*}
  where
  \begin{align*}
    \|E\|_2 
    & \leq |\sigma| \|F\|_2 + \|G_0\|_2 + \|A-\sigma B\|_2 \|G_3\|_2 + \|G_4\|_2 + \|G_5\|_2 + O(u^2) \\
    & \leq u |\sigma| (b_n + c_n + d_n + e_n + f_n) \eta \|X\|_2 \|B\|_2 + ua_n |\sigma| \|B\|_2 \\
    & \qquad + u(a_n + d_n+e_n+f_n) \|A-\sigma B\|_2 +O(u^2)\\
    & \leq u |\sigma_0| (b_n + c_n + d_n + e_n + f_n) \eta \|X\|_2 \|A\|_2 + ua_n |\sigma_0| \|A\|_2 \\
    & \qquad + u(a_n + d_n+e_n+f_n) (\|A\|_2 + |\sigma_0| \|A\|_2) +O(u^2)\\
    & \leq u \Big(|\sigma_0| (b_n + c_n + d_n + e_n + f_n) \eta \|X\|_2 \\
    & \qquad + (1+|\sigma_0|)(2a_n + d_n+e_n+f_n)\Big) \|A\|_2 +O(u^2).\\
  \end{align*}
\end{proof}

Algorithm~\ref{alg:gen_schur} computes a generalized Schur decomposition
from $B$ and $A-\sigma B$.  We define
\begin{equation}
  \label{eq:gamma_def}
  \gamma = \frac{\|A\|_2}{\|A-\sigma B\|_2}
\end{equation}
as a measure of catastrophic cancellation in forming $A-\sigma B$ that
might compromise stability.  Cancellation did not turn out to be a
concern in Theorem~\ref{th:generalized_schur}, but the quantity
$|\sigma_0| \gamma$ will appear in the residual bounds.  This quantity
is large only if $\gamma$ is large, regardless of the magnitude of
$|\sigma_0|$.  To see this note that
\begin{equation*}
  |\sigma_0| \gamma 
  \leq \frac{|\sigma_0| \|A\|_2}{| \|A\|_2 - |\sigma| \|B\|_2|}
  = \frac{|\sigma_0| \|A\|_2}{| \|A\|_2 - |\sigma_0| \|A\|_2|}
  = \frac{|\sigma_0|}{|1-|\sigma_0||}.
\end{equation*}
If $|\sigma_0| \leq 2$, then $|\sigma_0 \gamma| \leq 2\gamma$.  If
$|\sigma_0| > 2$, then the above inequality implies that
$|\sigma_0| \gamma \leq 2$.  Thus, with no assumption on $\sigma_0$,
we have
\begin{equation*}
  \label{eq:sigma0_gamma_bound}
  |\sigma_0|\gamma \leq 2\max(\gamma, 1).
\end{equation*}


\begin{theorem}
  \label{th:residual_bounds}
  Assume that \eqref{eq:shift_error} and \eqref{eq:Xerror} hold.
  Suppose that a computed eigenvalue $\theta\neq 0$ and eigenvector
  $\vec{v}$ of $X$ satisfy
  \begin{equation*}
    X\vec{v} - \theta \vec{v} = \vec{r},
    \qquad\text{with}\qquad
    \|\vec{r}\|_2 \leq u g_n \|X\|_2 \|\vec{v}\|_2.
  \end{equation*}
  Let $\lambda = \sigma + 1/\theta$, with $\lambda = \infty$ if
  $\theta = 0$.  Then
  \begin{multline}
    \label{eq:eq:res_bound_large_eig}
    \| \theta A \vec{v} - (1+\sigma \theta) B\vec{v} \|_2 \\
    \leq u |1+\sigma \theta| \cdot |1-\sigma/\lambda|
    \Big( b_n + g_n  + a_n (1+2\max(\gamma, 1))  \Big)\eta \|X\|_2\|B\|_2 \|\vec{v}\|_2 
    + O(u^2),
  \end{multline}
  where we take $|1-\sigma/\lambda| = 1$ if $\lambda = \infty$ and
  $|1-\sigma/\lambda| = \infty$ if $\lambda = 0$ so that the bound is
  vacuous if $\lambda = 0$.  We also have
  \begin{multline}
    \label{eq:eq:res_bound_small_eig}
    \| \theta A \vec{v} - (1+\sigma \theta) B\vec{v} \|_2 \\
    \leq u |\theta| \cdot |\sigma_0| \cdot |1-\lambda/\sigma|
    \Big( b_n + g_n  + a_n (1+2\max(\gamma, 1))  \Big)\eta \|X\|_2\|A\|_2 \|\vec{v}\|_2 
    + O(u^2),
  \end{multline}
\end{theorem}
\begin{proof}
  With $G_0$ and $G_1$ as defined in the proof of Theorem~\ref{th:generalized_schur} we have
  \begin{equation*}
    (\breve{A}-G_0)X \vec{v} - \theta(A-\sigma B) \vec{v} = (A-\sigma B)\vec{r},
  \end{equation*}
  \begin{equation*}
    (B + G_1 - G_0 X)\vec{v}- \theta(A-\sigma B) \vec{v} = (A-\sigma B)\vec{r},
  \end{equation*}
  and
  \begin{equation*}
    \theta A \vec{v} - (1+\sigma \theta) B\vec{v} = G_1\vec{} - G_0 X \vec{v} - (A-\sigma B)\vec{r}.
  \end{equation*}
  It follows that
  \begin{align}
    \| \theta A \vec{v} - (1+\sigma \theta) B\vec{v} \|_2 
    & \leq u \Big( b_n \|\breve{A}\|_2\|X\|_2 + a_n (\|\breve{A}\|_2
      + |\sigma| \|B\|_2) \|X\|_2 \nonumber\\
    & \qquad + g_n \|A-\sigma B\|_2 \|X\|_2 \Big) \|\vec{v}\|_2 \nonumber\\
    & = u \Big( (a_n+b_n+g_n) \eta \|X\|_2 \|B\|_2 
      + a_n |\sigma_0| \|A\|_2 \|X\|_2\Big) \|\vec{v}\|_2  \nonumber\\
    & \qquad + O(u^2) \nonumber\\
    & = u \Big( b_n + g_n  + a_n (1+|\sigma_0| \gamma)  \Big)\eta \|X\|_2\|B\|_2
      \|\vec{v}\|_2 \label{eq:res_proof_bound}.
  \end{align}
  We note that
  \begin{equation*}
    1+\sigma \theta 
    = 1 + \sigma \frac{1}{\lambda - \sigma} 
    = \frac{1}{1-\sigma/\lambda},
  \end{equation*}
  which together with \eqref{eq:res_proof_bound} and
  \eqref{eq:sigma0_gamma_bound} implies \eqref{eq:eq:res_bound_large_eig}.  With
  \begin{equation*}
    \theta 
    = \frac{1}{\lambda - \sigma} = \frac{\|B\|_2}{\sigma_0 \|A\|_2 (\lambda/\sigma - 1)}
  \end{equation*}
  we obtain
  \begin{equation*}
    \|B\|_2 = |\sigma_0(1- \lambda/\sigma)| \|A\|_2,
  \end{equation*}
  which substituted into \eqref{eq:res_proof_bound} gives
  \eqref{eq:eq:res_bound_small_eig}.
\end{proof}




\bibliography{/home/mas/work/bib/ref}


\end{document}
